[{"content":"Problem \u0026ldquo;Helm helps you manage Kubernetes applications — Helm Charts help you define, install, and upgrade even the most complex Kubernetes application. Charts are easy to create, version, share, and publish — so start using Helm and stop the copy-and-paste.\u0026rdquo; \u0026ndash; Helm\nHelm Charts are a fantastic tool to utilize if you are distributing your complex applications to a wide audience. However, it starts to become a chore to manage as the number of Applications(Charts) and Clusters increase. Without using a Continuous Delivery (CD) tool, such as ArgoCD, you are left to manage the Charts manually.\nWell that was until I stumbled across this Helm Plugin \u0026hellip;\nSolution Helm Whatup is a Helm plugin to help users determine if there\u0026rsquo;s an update available for their installed charts. It works by reading your locally cached index files from the chart repositories (via helm repo update) and checking the version against the latest deployed version of your charts in the Kubernetes cluster.\nNote: The original author of Helm Whatup has since stopped development. Thanks to the power of the community, fabmation-gmbh has forked the project and continued development.\nInstallation Perform the following steps to install Helm Whatup\nSet the HELM_HOME Environment Variable to the Helm Directory (MacOS) export HELM_HOME=/Users/{{username}}/Library/helm Install the Helm Plugin helm plugin install https://github.com/fabmation-gmbh/helm-whatup Usage To use this plugin, simply update the local chart repositories (ensures current versions are found) and perform the scan.\nUpdate Chart Repositories helm repo update Check for updatable Charts \u0026hellip; In Current Namespace\nhelm whatup In All Namespaces\nhelm whatup -A Additional Help This Command lists all releases which are outdated. By default, the output is printed in a Table but you can change this behavior with the \u0026#39;--output\u0026#39; Flag. You can enable all BETA features by executing: export HELM_WHATUP_BETA_FEATURES=true Usage: whatup [flags] Aliases: whatup, od Flags: -a, --all show all releases, not just the ones marked deployed or failed -A, --all-namespaces list releases across all namespaces --color [BETA] colorize/highlight the repositories from where the chart has been installed -d, --date sort by release date --deployed show deployed releases. If no other is specified, this will be automatically enabled --devel use development versions (alpha, beta, and release candidate releases), too. Equivalent to version \u0026#39;\u0026gt;0.0.0-0\u0026#39;. --failed show failed releases -h, --help help for whatup --ignore-deprecation ignore/skip charts which are marked as \u0026#34;DEPRECATED\u0026#34; (default true) --ignore-repo ignore error if no repo for a chart is found (default true) -m, --max int maximum number of releases to fetch (default 256) --offset int next release name in the list, used to offset from start value --only-source-updates only show updates of a chart repository where the Chart-Version and App-Version do match. (default true) -o, --output format prints the output in the specified format. Allowed values: table, json, yaml (default table) --pending show pending releases -r, --reverse reverse the sort order -q, --short output short (quiet) listing format --superseded show superseded releases --uninstalled show uninstalled releases --uninstalling show releases that are currently being uninstalled --version show version information ","permalink":"https://dodbrit.io/posts/simple-helm-update/","summary":"Problem \u0026ldquo;Helm helps you manage Kubernetes applications — Helm Charts help you define, install, and upgrade even the most complex Kubernetes application. Charts are easy to create, version, share, and publish — so start using Helm and stop the copy-and-paste.\u0026rdquo; \u0026ndash; Helm\nHelm Charts are a fantastic tool to utilize if you are distributing your complex applications to a wide audience. However, it starts to become a chore to manage as the number of Applications(Charts) and Clusters increase.","title":"Simple Helm Update"},{"content":"After you start playing around with Kubernetes, or even Red Hat OpenShift, you are potentially going to run into an issue that I’ve had to encounter many times. Sometimes it happens quickly, other times it takes longer, but eventually you will run into the issue of a namespace trying to terminate but appear to be stuck.\nThe TLDR of this issue, is that the Kubernetes Finalizer of the namespace fails to complete successfully leaving you with a namespace that is “terminating”.\nInfo\nAfter performing extensive research on this topic, modifying the Kubernetes Finalizer may cause unexpected results. There have been instances where the namespace was removed via the Kubernetes Finalizer and when creating a namespace of the same name, artifacts from the previous namespace remained (i.e. Pods ).\nIt is important to ensure that all artifacts (Pods, Services, etc.) have been removed prior to running this command.\nFrom what I have gathered (so far), the Kubernetes Finalizer is essentially the steps that need to be taken prior to a delete action. In this situation, it appears that the Kubernetes Finalizer is ensuring that all of the resources within a namespace have been deleted.\nIn a perfect world, you should track down why the Kubernetes Finalizer failed in the first place and resolve that issue. However, the world isn’t always perfect, and you don’t always have the time.\nThe following command is a quick, and dirty, way to remove that terminating namespace. In most situations, it will work as expected, but there is a chance that some strange things could happen. I personally have had resources recreated after terminating a namespace and creating a new namespace with the same name as the previous namespace.\nBefore running this command, be sure to replace “broken-app” with the namespace that you are trying to remove.\nexport ns=broken-app; kubectl get ns \u0026#34;${ns}\u0026#34; -o json | sed s/\\\u0026#34;kubernetes\\\u0026#34;//g | kubectl replace --raw /api/v1/namespaces/\u0026#34;${ns}\u0026#34;/finalize -f - ","permalink":"https://dodbrit.io/posts/2021-10-29/terminating-namespaces/","summary":"After you start playing around with Kubernetes, or even Red Hat OpenShift, you are potentially going to run into an issue that I’ve had to encounter many times. Sometimes it happens quickly, other times it takes longer, but eventually you will run into the issue of a namespace trying to terminate but appear to be stuck.\nThe TLDR of this issue, is that the Kubernetes Finalizer of the namespace fails to complete successfully leaving you with a namespace that is “terminating”.","title":"Terminating Namespaces"},{"content":"Having followed along with the previous three blog posts, you should have a cluster that is one small step away from demonstrating the storage capabilities. In this final part of the series, we will perform our final configurations and demonstrate the simplicity of our storage solution.\nNote\nAt the end of this post, your cluster isn’t quite ready for a workload as it is still missing an Ingress Controller. While deploying an Ingress Controller is outside the scope of this post, I will briefly describe how to deploy NGINX as an IngressController at the end of this post to get you started. I plan on making a post about this later to get into more detail.\nStep 1: Define Storage Policy in vSphere In order for our StorageClass to know where to store our persistent data, we are going to create a simple Storage Policy in vSphere. For the purposes of this demo, we are going to create a simple tag based policy. Feel free to make additional tags if your situation requires it. The names can be changed to suit your needs, but remember what you called them as you will need them later.\nvSphere Category Login to your vSphere instance. Click the Menu navigation dropdown and select Tags \u0026amp; Custom Attributes. Click Categories and Select New. Give the Category the name Kubernetes Storage, select One Tag, and deselect all but Datastore. Click Create. vSphere Tags Navigate back to Tags. Click New and create a new tag with the Demo, using Kubernetes Storage as the Category. Click Create. Tag vSphere Datastore Now we need to apply our newly created tag to our Datastore. This task is as simple as navigating to the Datastore tab, clicking Summary section, and selecting Assign in the Tags card. Select the newly created Demo tag and click Assign on last time to close the dialog box.\nvSphere Storage Policy Now that we have created our tags, and assigned them to our Datastore, we can create our simple but effective Storage Policy. Feel free to customize this later to suit your needs.\nClick the Menu navigation dropdown and select Policy and Profiles. On VM Storage Policies click Create to create a new Storage Policy. Name the policy Demo and click Next (remember this name as it is needed in the StorageClass step). For the Policy structure, select Enable tag based placement rules. Click Next. In this dialog, select the Category you created in the previous step and leave Usage Option with the defaulted Use storage tagged with. Click Browse and select the appropriate Tag. Click Next. Confirm that your tag rule worked by verifying the DataStore populated in this table. If they are correct click Next and then Finish. Step 2: Define Storage Class The StorageClass defines how Kubernetes handles requests for persistent storage. By defining the StorageClass it allows application deployed to the cluster the ability to request storage dynamically. While this may seem a little scary at first, it’s not the wild west. Limits can still be placed on a per application or per user basis.\n#vmware-sc.yml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: vsphere annotations: #storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; # Optional provisioner: csi.vsphere.vmware.com #allowVolumeExpansion: true # Optional: only applicable to vSphere 7.0U1 and above parameters: storagepolicyname: \u0026#34;Demo\u0026#34; #Optional Parameter In the above file;\nAttribute Description metadata/name\tDefines the name of the StorageClass. This is the name used by your application deployments when requesting storage metadata/annotations/storageclass.kubernetes.io/is-default-class This is set to true if you wish this Storage Class to be set as the default for the cluster provisioner This tells Kubernetes to use the CSI driver we deployed in the previous blog allowVolumeExpansion This is in beta. For more information read the documentation parameters/storagepolicyname This is wait instructs the CSI to use the Storage Policy to find the correct Datastore We can now apply our StorageClass to the cluster and we can validated that it applied successfully;\n## APPLY STORAGE CLASS kubectl apply -f vmware-sc.yml ## VALIDATE kubectl get sc This command should return two items; local-path and vsphere.\nDemo: Usage of vSphere Storage Without going into a deep dive in the realm of storage, the different types, Block vs. File, I’m going to demonstrate how to use Block Storage on vSphere. For the vast majority of Kubernetes workloads Block Storage will work great.\nIf you have a workload that requires File Storage, review the documentation here to view the additional items that are required in vSphere to make that work.\nAs previously mentioned, one of the benefits of using a StorageClass is the ability to dynamically provision persistent volumes.\n## example-storage.yml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: vsphere-demo-labels labels: app: mongoDB environment: test spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: vsphere Using the example-storage.yml file defined above, running kubectl apply -f example-storage.yml will result in a PersistentVolumeClaim being created and a matching VMDK within vSphere.\nTo view your Persistent Volume Claims, simply run kubectl get pvc. You may need to add -n {{ NAMESPACE_NAME }} if you are deploying your volume in a namespace other than default.\nAdditionally, you can view your Persistent storage from within vSphere. If you navigate to Storage -\u0026gt; Name of DataStore -\u0026gt; Monitor. You can then select Cloud Native Storage -\u0026gt; Container Volumes. This will list of all the volumes being used on that Datastore.\nYou can add, or remove, labels in the Metadata section of the example-storage.yml file. These tags will appear within the vSphere UI and allow for easier filtering and deciphering of what application is using what volume.\nThe final thing to mention is, while this only an example Volume Claim, when deploying a production application, the same principles apply. Your application should have a PersistentVolumeClaim , if not multiple, and the only updates you should have to do is add the StorageClassName and possibly tags (if they aren’t already there).\nSummary While there were quite a few steps to get your cluster configured for use with your vSphere Infrastructure, at the end of the day, the struggle should be worth it. By utilizing your vSphere infrastructure, you can leverage the same backup and restore procedures to backup your persistent data.\nThank you for taking the time to read my blog and bearing with me while I figure out this whole blogging thing. Please feel free to submit any comments below on ways to improve.\nCheers,\nPeter\nReferences Rancher Labs Longhorn vSphere Cloud Provider vSphere Storage Provider Rancher Labs K3S Kuberenetes Cloud Controllers VMware Photon OS NGINX Load Balancer Govc Documentation ","permalink":"https://dodbrit.io/posts/2021-07-03/k3s-cluster-with-vsphere-storage-part-iv/","summary":"Having followed along with the previous three blog posts, you should have a cluster that is one small step away from demonstrating the storage capabilities. In this final part of the series, we will perform our final configurations and demonstrate the simplicity of our storage solution.\nNote\nAt the end of this post, your cluster isn’t quite ready for a workload as it is still missing an Ingress Controller. While deploying an Ingress Controller is outside the scope of this post, I will briefly describe how to deploy NGINX as an IngressController at the end of this post to get you started.","title":"K3s Cluster With Vsphere Storage [Part IV]"},{"content":"Having followed the steps in Part II, you should have a K3S Cluster stood up. In this Part we are going to add the all important Cloud Provider.\nStep 1: Prepare Nodes for vSphere Cloud Provider Before we can install the vSphere Cloud provider, there are couple configurations that need to be applied. You can read more about these requirements by heading over to the VMWare documentation.\nTaint Nodes The Cloud Provider is going to be installed on the Server (Master) Nodes. To ensure this happens correctly, we need to ensure the nodes are tainted correctly.\nInfo\nNot sure if this is a K3S quirk or just my luck, but additionally I had to reapply the Master and Worker roles to the Nodes for the DaemonSet to recognize the Role.\n## TAINT SERVER (MASTER) NODES kubectl taint nodes --selector=\u0026#39;node-role.kubernetes.io/master\u0026#39; node-role.kubernetes.io/master=:NoSchedule ## ADD ROLES TO SERVER (MASTER) NODES kubectl label nodes --selector=\u0026#39;node-role.kubernetes.io/master\u0026#39; node-role.kubernetes.io/master= --overwrite ## TAINT AGENT (WORKER) NODES kubectl taint nodes --selector=\u0026#39;!node-role.kubernetes.io/master\u0026#39; node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule ## ADD ROLES TO AGENT (WORKER) NODES kubectl label nodes --selector=\u0026#39;node-role.kubernetes.io/worker\u0026#39; node-role.kubernetes.io/worker= --overwrite Once you have applied the Taints, you can verify they applied successfully by running this command;\n## VALIDATE TAINTS kubectl describe nodes | egrep \u0026#34;Taints:|Name:\u0026#34; If the taints applied successfully you should see all the Server (Master) Nodes with the Taint node-role.kubernetes.io/master:NoSchedule and all Agent (Worker) Nodes have the Taint node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule\nConfigure Virtual Machine Settings Another configuration item that needs to occur, is ensuring that each virtual machine’s hard disk (vmdk) is assigned a unique identifier (UUID). The easiest way to interact with the virtual machine settings is to use a command line utility called govc.\ngovc is vSphere CLI provided by VMware and is built on top of govmoi. The CLI is designed to be a user friendly CLI alternative to the GUI and well suited for automation tasks. It also acts as a test harness for the govmomi APIs and provides working examples of how to use the APIs.\n– Govc Github\nTo install the CLI on Mac, you can use Homebrew. To install govc run brew install govc. For other operating systems, please refer to the documentation.\nGOVC Setup Before we can use the utility we have to specify how govc connects to vSphere. In Mac and Linux, this is accomplished by setting environment variables.\n## CONFIGURE GOVC export GOVC_URL=\u0026#39;{{ URL-FOR-VCSA }}\u0026#39; export GOVC_USERNAME=\u0026#39;administrator@vsphere.local\u0026#39; export GOVC_PASSWORD=\u0026#39;{{ PASSWORD-FOR-ABOVE-ACCOUNT }}\u0026#39; export GOVC_INSECURE=1 Variable Description GOVC_URL the URL for your vSphere (VCSA) instance GOVC_USERNAME the login account for vSphere GOVC_PASSWORD the password for the account specified GOVC_INSECURE used when self-signed certificates are in use on VCSA, suppresses certificate warnings Enable DiskUUID At this point you should be able to utilize govc to query vSphere. The first command you can run is govc ls. This will list the any DataCenters you have configured and folders. You will need to enable DiskUUID on all the virtual machines in the cluster and to do this you will need to know the path to the virtual machine.\nTo find the path use the govc ls command followed by a path. Start with leaving it blank and keep drilling down until you find the paths to your virtual machines. For the example below, my final govc command looked like; govc ls /Homelab/vm/Applications/Demo/\nEnable DiskUUID on the virtual machines by running the following command. Outline below is how I applied it in my stack.\n## ADD DISK UUID FLAG govc vm.change -e=\u0026#34;disk.enableUUID=1\u0026#34; -vm=\u0026#39;{{ PATH-TO-VM }}\u0026#39; ## EXAMPLE -- ADD DISK UUID govc vm.change -e=\u0026#34;disk.enableUUID=1\u0026#34; -vm=\u0026#39;/Homelab/vm/Applications/Demo/MASTER-001\u0026#39; govc vm.change -e=\u0026#34;disk.enableUUID=1\u0026#34; -vm=\u0026#39;/Homelab/vm/Applications/Demo/MASTER-002\u0026#39; govc vm.change -e=\u0026#34;disk.enableUUID=1\u0026#34; -vm=\u0026#39;/Homelab/vm/Applications/Demo/MASTER-003\u0026#39; govc vm.change -e=\u0026#34;disk.enableUUID=1\u0026#34; -vm=\u0026#39;/Homelab/vm/Applications/Demo/WORKER-001\u0026#39; govc vm.change -e=\u0026#34;disk.enableUUID=1\u0026#34; -vm=\u0026#39;/Homelab/vm/Applications/Demo/WORKER-002\u0026#39; govc vm.change -e=\u0026#34;disk.enableUUID=1\u0026#34; -vm=\u0026#39;/Homelab/vm/Applications/Demo/WORKER-003\u0026#39; Adding ProviderID The final configuration change needed is to add a “ProviderID” to each of the nodes. Reading though the documentation, this ID needs to be unique but can be set to anything (within reason). Borrowing from the documentation, the easiest thing to do is to assign the virtual machine UUID as the ProviderID as these are always unique.\n## ADD PROVIDERID TO EACH NODE for vm in $(govc ls /Homelab/vm/Applications/Demo/); do MACHINE_INFO=$(govc vm.info -json -dc=Homelab -vm.ipath=\u0026#34;$vm\u0026#34; -e=true) VM_NAME=$(jq -r \u0026#39; .VirtualMachines[] | .Name\u0026#39; \u0026lt;\u0026lt;\u0026lt; $MACHINE_INFO | awk \u0026#39;{print tolower($0)}\u0026#39;) VM_UUID=$( jq -r \u0026#39; .VirtualMachines[] | .Config.Uuid\u0026#39; \u0026lt;\u0026lt;\u0026lt; $MACHINE_INFO | awk \u0026#39;{print toupper($0)}\u0026#39;) kubectl patch node $VM_NAME.dodbrit.lab -p \u0026#34;{\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;providerID\\\u0026#34;:\\\u0026#34;vsphere://$VM_UUID\\\u0026#34;}}\u0026#34;; done For good measure, you can validate that the ProviderIDs were added correctly by running kubectl describe nodes | egrep \u0026quot;ProviderID:|Name:\u0026quot;. The expected outcome should be all of the nodes listed, with a ProviderID that starts with “vsphere”. Additionally, the IDs should all be unique.\nStep 2: Install vSphere CPI Now that we have our cluster up and running, and the nodes configured with additional information, we can finally install the vSphere Cloud Provider.\nThe first step is generate the required configuration file (vsphere.conf) and credentials (cpi-secret.yaml).\n# vsphere.conf # Global properties in this section will be used for all specified vCenters unless overriden in VirtualCenter section. global: # default https port port: 443 # set insecureFlag to true if the vCenter uses a self-signed cert insecureFlag: true # settings for using k8s secret secretName: cpi-global-secret secretNamespace: kube-system # vcenter section vcenter: # arbitrary name for cluster demo: # ip or fqdn of vcsa server: 10.0.15.5 # vSphere Datacenter Name datacenters: - Homelab # secret with credentials secretName: cpi-secret secretNamespace: kube-system # cpi-secret.yaml apiVersion: v1 kind: Secret metadata: name: cpi-secret namespace: kube-system stringData: 10.0.15.5.username: \u0026#34;administrator@vsphere.local\u0026#34; 10.0.15.5.password: \u0026#34;{{ PASSWORD-TO-ACCOUNT }}\u0026#34; To apply the vSphere configuration to the cluster, we will create a ConfigMap of the file. To do that you run the following command;\nkubectl create configmap cloud-config --from-file=vsphere.conf --namespace=kube-system This will create a ConfigMap in the kube-system namespace with all the items we defined. To add the login credentials, we just need to apply the Secret as that was defined as a Secret to begin with.\nkubectl create -f cpi-secret.yaml Again for peace of mind, we can validate these applied;\n## ENSURE CONFIGMAP WAS CREATED kubectl get configmap cloud-config --namespace=kube-system ## ENSURE SECRET WAS CREATED kubectl get secret cpi-secret --namespace=kube-system After you have verified that the ConfigMap and Secret was created, you can delete both of these files. These files have potentially sensitive information in them and its good practice to remove them.\nNow we can deploy all the components of the vSphere Cloud Provider. We are going to apply them directly from the vSphere GitHub repository.\nNote\nIt is recommend that you review the files first and once you feel confident that the files are safe, you can then apply them\n## CREATE ROLES kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/master/manifests/controller-manager/cloud-controller-manager-roles.yaml ## CREATE ROLE BINDINGS kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/master/manifests/controller-manager/cloud-controller-manager-role-bindings.yaml ## CREATE DAMEONSET kubectl apply -f https://github.com/kubernetes/cloud-provider-vsphere/raw/master/manifests/controller-manager/vsphere-cloud-controller-manager-ds.yaml And we can check that the Cloud Provider deployed successfully. You should see a vsphere-cloud-controller-manager running on each of your master nodes.\nkubectl get pods --all-namespaces Congratulations! We have just install the vSphere Cloud Provider within our cluster!\nStep 3: Install vSphere CSI Now that the Cloud Provider has been installed, we can turn the attention to the Cloud Storage Interface (CSI). Just like the Cloud Provider, we need to create some configuration files. Modify the file outlined below and save it as csi-vsphere.conf.\n[Global] cluster-id = \u0026#34;k3s-cluster\u0026#34; user = \u0026#34;administrator@vsphere.local\u0026#34; password = \u0026#34;{{ PASSWORD-FOR-ACCOUNT }}\u0026#34; port = \u0026#34;443\u0026#34; insecure-flag = \u0026#34;1\u0026#34; [VirtualCenter \u0026#34;{{ VCSA-IP-ADDRESS }}\u0026#34;] datacenters = \u0026#34;Homelab\u0026#34; [Workspace] server = \u0026#34;{{ VCSA-IP-ADDRESS }}\u0026#34; datacenter = \u0026#34;Homelab\u0026#34; default-datastore = \u0026#34;{{ DEFAULT-VSPHERE-DATASTORE }}\u0026#34; resourcepool-path = \u0026#34;{{ DATACENTER-NAME }}/Resources\u0026#34; folder = \u0026#34;kubernetes\u0026#34; [Disk] scsicontrollertype = pvscsi The default-datastore field in the configuration file is important enough that it is included in the file, but will not be used once we configure the Storage Class. The Storage Class grants us the ability to define where the persistent storage will be save. Additionally, multiple Storage Class‘s can be defined that will allow for more granular storage based upon the deployment.\nUnlike CPI, we are going to upload this configuration to Kubernetes via a Secret.\nkubectl create secret generic vsphere-config-secret --from-file=csi-vsphere.conf --namespace=kube-system When then validate that the Secret created successfully …\nkubectl get secret vsphere-config-secret --namespace=kube-system Note\nAt this point, you made delete csi-vsphere.conf as it contains sensitive information.\nNow we can deploy all the components of the vSphere Storage Provider. We are going to apply them directly from the vSphere GitHub repository.\n## DEFINE CLUSTER ROLES kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/v2.2.0/manifests/v2.2.0/rbac/vsphere-csi-controller-rbac.yaml ## DEFINE ROLE BINDINGS kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/v2.2.0/manifests/v2.2.0/rbac/vsphere-csi-node-rbac.yaml ## DEPLOY STORAGE DRIVERS (CONTROLLER) kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/v2.2.0/manifests/v2.2.0/deploy/vsphere-csi-controller-deployment.yaml ## DEPLOY STORAGE DRIVERS (DAMEONSET) kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/v2.2.0/manifests/v2.2.0/deploy/vsphere-csi-node-ds.yaml We can then validate the installation by checking the Pod deployment. It may take a few minutes for all of the pods to get created and start running.\nkubectl get pods --namespace=kube-system Summary At this point, and ensuring that everything went smoothly, you should have a Kubernetes cluster running with both vSphere CPI and CSI running and configured. In Part IV of this blog, we are going to define the storage class within the cluster and deploy a demo application.\nReferences Rancher Labs Longhorn vSphere Cloud Provider vSphere Storage Provider Rancher Labs K3S Kuberenetes Cloud Controllers VMware Photon OS NGINX Load Balancer Govc Documentation ","permalink":"https://dodbrit.io/posts/2021-07-03/k3s-cluster-with-vsphere-storage-part-iii/","summary":"Having followed the steps in Part II, you should have a K3S Cluster stood up. In this Part we are going to add the all important Cloud Provider.\nStep 1: Prepare Nodes for vSphere Cloud Provider Before we can install the vSphere Cloud provider, there are couple configurations that need to be applied. You can read more about these requirements by heading over to the VMWare documentation.\nTaint Nodes The Cloud Provider is going to be installed on the Server (Master) Nodes.","title":"K3s Cluster With Vsphere Storage [Part III]"},{"content":"Continuing on the discussion from Part I, in this part we are going to deploy a K3S cluster on a vSphere environment.\nStep 1: The Setup Note\nI’m currently in the process of creating Terraform scripts to accomplish this deployment, but feel free to deploy you cluster per your favorite method. I will create a blog post describing my Terraform scripts and the headache that PhotonOS caused once I’ve completed the Terraform Script.\nThe first thing we need to do is setup some nodes to deploy our K3S cluster too. Because I like pain, I decided to do this with VMware’s PhotonOS but you can use something much simpler like Ubuntu.\nFor this blog, I’m deploying; 3x VMs to act as Server (Master) nodes, 3x VMs to act as Agents (Worker) nodes and 1x VM as a cluster load balancer. At a minimum, each Virtual Machine (VM) needs to have a static IP, be able to communicate to one another and be able to communicate to the vSphere server (IP or FQDN). For complete setup instructions, refer to Rancher’s K3S documentation to find out what requirements your distribution of Linux needs to deploy K3S.\nThe hardware requirements for each node is going to be specific to what you wish to deploy within the cluster. Because workloads vary dramatically, it is very difficult to suggest hardware sizes. However, as a starting point, and assuming you have the hardware capacity, I typically start with 2 vCPUs, 4GB of RAM and 16GB HDD (Thin Provisioned) and grow from there.\nInfo\nIf you are using PhotonOS like I did, you will need to install AppArmor with the following command:\ndnf install -y apparmor-utils\nOnce you have deployed your Virtual Machines you should something similar to this. These IP addresses can be whatever suits your network as long as they can communicate to each other.\nNode Name IP Address Description MASTER-001 10.0.15.201 Server (Master) Node MASTER-002 10.0.15.202 Server (Master) Node MASTER-003 10.0.15.203 Server (Master) Node WORKER-001 10.0.15.204 Agent (Worker) Node WORKER-002 10.0.15.205 Agent (Worker) Node WORKER-003 10.0.15.206 Agent (Worker) Node LB-001 10.0.15.200 NGINX Load Balancer Step 2: Deploying the External Load Balancer To make life a whole lot easier, and to follow good practices, we are going to deploy an external load balancer for the cluster. In a production environment this can be a physical appliance such as an F5, but in this example we are going to deploy NGINX. This Load Balancer is going to be configured to direct all Kubernetes Management traffic to the Server (Master) Nodes, and all the HTTP/HTTPS traffic to the Agent (Worker) Nodes.\nNote\nIn a Homelab, or resource limited, environment, instead of deploying an external loadbalancer, you can deploy MetalLB within you cluster.\nIn the future, after the cluster and application(s) have been deployed, DNS records will be pointed to the IP address of this Load Balancer. That traffic will be router to the cluster where an Ingress Controller will route the traffic to the appropriate services and pods.\nDeploying the Load Balancer may vary depending upon your distribution of choice, but to keep this simple, all we are going to do is; install Docker on the node and deploy NGINX via Docker Compose. The following steps need to be performed on the Load Balancer node.\nFor PhotonOS, Docker is pre-installed. To utilize Docker on PhotonOS, you need to enable and start the Docker service.\n## ENABLE DOCKER SERVICE systemctl enable docker.service ## START DOCKER SERVICE systemctl start docker.service Info\nAs of 30 July 2021, PhotonOS 4.0 (build 1526e30ba) ships with an older version of Docker (Docker version 19.03.15, build 99e3ed8). Because of this we will need to install Docker-Compose.\n## INSTALL DOCKER-COMPOSE BINARY curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose ## MAKE EXECUTABLE chmod +x /usr/local/bin/docker-compose Now that we have our dependencies taken care of, the last thing we need to do is create some configurations files. The first one is docker-compose.yml and it is simply deploying a NGINX image with the configuration file that we are about to write. Additionally, the configuration specifies that NGINX shall use the local configuration file (nginx.conf) and to expect traffic on ports; 80, 443, 6443.\nNote\nDo NOT use “latest” tags in production. This is bad practice and can lead to unexpected outages and issues. By default, not specifying a tag, latest will be used.\nGiven this is in a Homelab, we can ignore this best practice.\n## DEFINE VERSION version: \u0026#34;3.7\u0026#34; ## DEFINE SERVICE services: nginx: image: nginx restart: always volumes: - ./nginx.conf:/etc/nginx/nginx.conf ports: - 80:80 - 443:443 - 6443:6443 Before we can start our NGINX image, we have to create the NGINX configuration. The file below is a simple configuration file that routes all HTTP/HTTPS traffic to the Agent (Worker) nodes and all Kubernetes Management traffic the Server (Master) nodes.\nUpdate the IP addresses to reflect the addresses you have specified in your environment.\n","permalink":"https://dodbrit.io/posts/2021-07-03/k3s-cluster-with-vsphere-storage-part-ii/","summary":"Continuing on the discussion from Part I, in this part we are going to deploy a K3S cluster on a vSphere environment.\nStep 1: The Setup Note\nI’m currently in the process of creating Terraform scripts to accomplish this deployment, but feel free to deploy you cluster per your favorite method. I will create a blog post describing my Terraform scripts and the headache that PhotonOS caused once I’ve completed the Terraform Script.","title":"K3s Cluster With Vsphere Storage [Part II]"},{"content":"Whether you work in an environment that utilizes VMware products, you have a VMware Homelab or you just want to learn something different for Kubernetes Storage this post is for you.\nWhen deploying a highly available Kubernetes cluster, distributed storage becomes a thorn in your side. Just like every other Cloud Native tool, there are a number of solutions out there; Rancher Lab’s Longhorn and GlusterFS just to name two. However, did you know, starting with vSphere 6.7u3 and newer, you can utilize your existing vSphere Datastore as persistent storage for your Kubernetes clusters?\nBeing a huge fan of Rancher Labs, and K3S, this blog will describe how to deploy a K3S cluster on vSphere and utilize vSphere Datastores as persistent storage.\nWhy write this blog? I’ll begin with openly admitting that i’m no William Shakespeare. I’m not the best at writing and i’m even worse at spelling (thank you spell check) but I have a way with simplifying a tough technical subject so that I can, and even management can, understand the task.\nSo, I decided to write this blog post for two main reason; firstly, and most selfishly, I wrote it for me. I guarantee there is going to be a time in my future where I’m going to need to understand how I accomplished this task. While writing documentation is a pain, it does provide very useful from time to time and as an added bonus, by writing down the steps and researching the technology behind it, I gained additional knowledge and have a better chance at remembering it too. If it just so happens that the post helps you too, well then thats another great bonus.\nThe second reason I wrote this blog is because I really didn’t find much out there talking about the same topic. Anytime I’ve research Kubernetes Storage, it resulted in deploy X or install Y. In the same fashion, anytime you research anything Kubernetes related it will involve something cloud related i.e. use AWS S3 buckets. While I’ll believe AWS has a time and a place, I like to do things on-prem and in Homelabs too.\nHow does this all work? Before we get into the fun stuff, I thought I would explain what is going to happen and how it allows us to utilize vSphere resources. Feel free to skip this if you want as this is more of an FYI if you ever need to deploy this in a secured production environment and have to explain yourself.\nTraditionally Kubernetes (K8s) deploys with “In-Tree” cloud providers. The simplest way i’ve understood this is; providers are essentially drivers within Kubernetes that allow communication with cloud resources; for example AWS ELBs. They can essentially translate Kubernetes commands into the appropriate cloud API calls. Using the AWS ELB example, it could, in theory, update the ELB to include a new host IP.\nWith K3S, Rancher Labs manages to minimize the binary by removing these providers and including a lightweight “local” cloud provider. K3S can be deployed in small environments, on IOT devices, on the Edge and even in space. In these situations, Cloud providers aren’t needed. Just imagine the bill trying to reach AWS services from space. With all that being said, K3S can still be deployed in a production environment and can still utilize other cloud providers.\nWhen adding additional Cloud Providers to any Kubernetes distribution, it is known as “Out-of-Tree” Cloud Providers. vSphere provides both the Cloud Provider Interface (CPI) and Container Storage Interface (CSI) for install in this method.\nNote\nTraditionally Kubernetes recommended using In-Tree providers. However, recently there has been a shift to using Out-of-Tree providers by default.\nTerminology Term Definition Cloud Provider Interface (CPI) The Brains of the Provider. This is what makes the connection between the Kubernetes Cluster and the Cloud Provider Container Storage Interface (CSI) Using the CPI, this allows Kubernetes Storage Classes the ability to communicate with and utilize cloud tools. For example, vSphere Datastore In-Tree Providers Providers natively shipped with Kubernetes Out-of-Tree Providers Providers that are installed into Kubernetes after deployment Summary While this blog was filled with dry information, it definitely helps describe what the heck is going on. In the next part of this blog, I will demonstrate the deployment of a K3S cluster in a vSphere environment.\nReferences Rancher Labs Longhorn vSphere Cloud Provider Rancher Labs K3S Kuberenetes Cloud Controllers ","permalink":"https://dodbrit.io/posts/2021-07-03/k3s-cluster-with-vsphere-storage-part-i/","summary":"Whether you work in an environment that utilizes VMware products, you have a VMware Homelab or you just want to learn something different for Kubernetes Storage this post is for you.\nWhen deploying a highly available Kubernetes cluster, distributed storage becomes a thorn in your side. Just like every other Cloud Native tool, there are a number of solutions out there; Rancher Lab’s Longhorn and GlusterFS just to name two. However, did you know, starting with vSphere 6.","title":"K3s Cluster With Vsphere Storage [Part I]"},{"content":" Good afternoon and happy Friday! Thanks to the wonderful Network Chuck, today is day one of this website. As a beginning post I thought I would start with a free DevOps certification.\nNote\nAs a quick update, I have successfully passed the Rancher Operator: Level One!\nAs I previously mentioned, despite the fact that this is a Rancher certificate that is obviously geared towards the Rancher product, I highly recommend this certification for anyone that is new to the Kubernetes world or needs a refresher for the matter.\nRancher Operator: Level One touches upon many of the core concepts of Kubernetes and is a great place to start.\nRancher is one of the leading companies when it comes to all things DevOps (they haven’t paid me to say that). As a commitment to the community, late last year Rancher released a no cost certification for their Rancher platform. While the certification is about Rancher’s cluster management tool, the certification is a great entry into the world of Kubernetes.\nhttps://academy.rancher.com/\nHopefully before my next post, I will be Rancher Operator certified!\nHave a great weekend everyone!\nCheers,\nPeter\n","permalink":"https://dodbrit.io/posts/day-one/","summary":"Good afternoon and happy Friday! Thanks to the wonderful Network Chuck, today is day one of this website. As a beginning post I thought I would start with a free DevOps certification.\nNote\nAs a quick update, I have successfully passed the Rancher Operator: Level One!\nAs I previously mentioned, despite the fact that this is a Rancher certificate that is obviously geared towards the Rancher product, I highly recommend this certification for anyone that is new to the Kubernetes world or needs a refresher for the matter.","title":"Day One"},{"content":" Welcome! As a quick introduction, my name is Peter Keech. I’m a DevOps (DevSecOps) Engineer working in the government sector and a U.S. Air Force Veteran. I have a passion for all things DevOps and I am always learning new things about the technology.\nThe goal of this blog is to hopefully become a collection of lessons learnt throughout my journey in aiding the government in adopting DevOps policies and practices, as well as documenting my own growth along the way.\nHopefully, some of the lessons learnt here will help you in achieving the goals you have set out to complete.\nCheers for taking the time to visit my blog.\n","permalink":"https://dodbrit.io/about/","summary":"About Me","title":"About Me"}]