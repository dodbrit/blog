"use strict";(self.webpackChunkapp=self.webpackChunkapp||[]).push([[9450],{6029:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"2021-10-29/terminating-namespaces","metadata":{"permalink":"/2021-10-29/terminating-namespaces","source":"@site/blog/2021-10-29/index.md","title":"Terminating Namespaces","description":"Terminating Namespaces Hero","date":"2021-10-29T00:00:00.000Z","formattedDate":"October 29, 2021","tags":[{"label":"Kubernetes","permalink":"/tags/kubernetes"},{"label":"Troubleshooting","permalink":"/tags/troubleshooting"}],"readingTime":1.64,"hasTruncateMarker":true,"authors":[{"name":"Peter Keech","title":"DevOps Engineer","url":"https://github.com/pkeech","imageURL":"https://github.com/pkeech.png","key":"Peter"}],"frontMatter":{"slug":"2021-10-29/terminating-namespaces","title":"Terminating Namespaces","authors":["Peter"],"tags":["Kubernetes","Troubleshooting"]},"nextItem":{"title":"K3S Cluster with vSphere Storage (Part I)","permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-i"}},"content":"![Terminating Namespaces Hero](TerminatingNamespaces.png)\\n\\nAfter you start playing around with *Kubernetes*, or even *Red Hat OpenShif*t, you are potentially going to run into an issue that I\u2019ve had to encounter many times. Sometimes it happens quickly, other times it takes longer, but eventually you will run into the issue of a namespace trying to terminate but appear to be stuck.\\n\\n\x3c!--truncate--\x3e\\n\\nThe TLDR of this issue, is that the **Kubernetes Finalizer** of the namespace fails to complete successfully leaving you with a namespace that is \u201cterminating\u201d.\\n\\n:::caution\\n\\nAfter performing extensive research on this topic, modifying the **Kubernetes Finalizer** may cause unexpected results. There have been instances where the *namespace* was removed via the **Kubernetes Finalizer** and when creating a namespace of the same name, artifacts from the previous namespace remained (i.e. Pods ). \\n\\nIt is important to ensure that all artifacts (Pods, Services, etc.) have been removed prior to running this command.\\n \\n:::\\n\\nFrom what I have gathered (so far), the **Kubernetes Finalizer** is essentially the steps that need to be taken prior to a delete action. In this situation, it appears that the Kubernetes Finalizer is ensuring that all of the resources within a namespace have been deleted.\\n\\nIn a perfect world, you should track down why the Kubernetes Finalizer failed in the first place and resolve that issue. However, the world isn\u2019t always perfect, and you don\u2019t always have the time. \\n\\nThe following command is a quick, and dirty, way to remove that terminating namespace. In most situations, it will work as expected, but there is a chance that some strange things could happen. I personally have had resources recreated after terminating a namespace and creating a new namespace with the same name as the previous namespace.\\n\\n**Before running this command, be sure to replace \u201cbroken-app\u201d with the namespace that you are trying to remove.**\\n\\n``` bash showLineNumbers\\nexport ns=broken-app; kubectl get ns \\"${ns}\\" -o json | sed s/\\\\\\"kubernetes\\\\\\"//g | kubectl replace --raw /api/v1/namespaces/\\"${ns}\\"/finalize -f -\\n```"},{"id":"2021-07-03/k3s-cluster-with-vsphere-storage-part-i","metadata":{"permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-i","source":"@site/blog/2021-07-30-k3s-vsphere-part1/index.md","title":"K3S Cluster with vSphere Storage (Part I)","description":"K3S Cluster with vSphere Storage Hero","date":"2021-07-30T00:00:00.000Z","formattedDate":"July 30, 2021","tags":[{"label":"Rancher","permalink":"/tags/rancher"},{"label":"Kubernetes","permalink":"/tags/kubernetes"},{"label":"Storage","permalink":"/tags/storage"},{"label":"K3S","permalink":"/tags/k-3-s"},{"label":"VMWare","permalink":"/tags/vm-ware"}],"readingTime":3.84,"hasTruncateMarker":true,"authors":[{"name":"Peter Keech","title":"DevOps Engineer","url":"https://github.com/pkeech","imageURL":"https://github.com/pkeech.png","key":"Peter"}],"frontMatter":{"slug":"2021-07-03/k3s-cluster-with-vsphere-storage-part-i","title":"K3S Cluster with vSphere Storage (Part I)","authors":["Peter"],"tags":["Rancher","Kubernetes","Storage","K3S","VMWare"],"image":"k3s_vsphere_part1.png","comments":true},"prevItem":{"title":"Terminating Namespaces","permalink":"/2021-10-29/terminating-namespaces"},"nextItem":{"title":"K3S Cluster with vSphere Storage (Part II)","permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-ii"}},"content":"![ K3S Cluster with vSphere Storage Hero](k3s_vsphere_part1.png)\\nWhether you work in an environment that utilizes VMware products, you have a VMware Homelab or you just want to learn something different for Kubernetes Storage this post is for you.\\n\\n\x3c!--truncate--\x3e\\n\\nWhen deploying a highly available Kubernetes cluster, distributed storage becomes a thorn in your side. Just like every other Cloud Native tool, there are a number of solutions out there; Rancher Lab\u2019s Longhorn and GlusterFS just to name two. However, did you know, starting with vSphere 6.7u3 and newer, you can utilize your existing vSphere Datastore as persistent storage for your Kubernetes clusters?\\n\\nBeing a huge fan of Rancher Labs, and K3S, this blog will describe how to deploy a K3S cluster on vSphere and utilize vSphere Datastores as persistent storage.\\n\\n## Why write this blog?\\nI\u2019ll begin with openly admitting that i\u2019m no William Shakespeare. I\u2019m not the best at writing and i\u2019m even worse at spelling (thank you spell check) but I have a way with simplifying a tough technical subject so that I can, and even management can, understand the task.\\n\\nSo, I decided to write this blog post for two main reason; firstly, and most selfishly, I wrote it for me. I guarantee there is going to be a time in my future where I\u2019m going to need to understand how I accomplished this task. While writing documentation is a pain, it does provide very useful from time to time and as an added bonus, by writing down the steps and researching the technology behind it, I gained additional knowledge and have a better chance at remembering it too. If it just so happens that the post helps you too, well then thats another great bonus.\\n\\nThe second reason I wrote this blog is because I really didn\u2019t find much out there talking about the same topic. Anytime I\u2019ve research Kubernetes Storage, it resulted in deploy X or install Y. In the same fashion, anytime you research anything Kubernetes related it will involve something cloud related i.e. use AWS S3 buckets. While I\u2019ll believe AWS has a time and a place, I like to do things on-prem and in Homelabs too.\\n\\n## How does this all work?\\nBefore we get into the fun stuff, I thought I would explain what is going to happen and how it allows us to utilize vSphere resources. Feel free to skip this if you want as this is more of an FYI if you ever need to deploy this in a secured production environment and have to explain yourself.\\n\\nTraditionally Kubernetes (K8s) deploys with \u201cIn-Tree\u201d cloud providers. The simplest way i\u2019ve understood this is; providers are essentially drivers within Kubernetes that allow communication with cloud resources; for example AWS ELBs. They can essentially translate Kubernetes commands into the appropriate cloud API calls. Using the AWS ELB example, it could, in theory, update the ELB to include a new host IP.\\n\\nWith K3S, Rancher Labs manages to minimize the binary by removing these providers and including a lightweight \u201clocal\u201d cloud provider. K3S can be deployed in small environments, on IOT devices, on the Edge and even in space. In these situations, Cloud providers aren\u2019t needed. Just imagine the bill trying to reach AWS services from space. With all that being said, K3S can still be deployed in a production environment and can still utilize other cloud providers.\\n\\nWhen adding additional Cloud Providers to any Kubernetes distribution, it is known as \u201cOut-of-Tree\u201d Cloud Providers. vSphere provides both the Cloud Provider Interface (CPI) and Container Storage Interface (CSI) for install in this method.\\n\\n:::tip\\nTraditionally Kubernetes recommended using In-Tree providers. However, recently there has been a shift to using Out-of-Tree providers by default.\\n:::\\n\\n## Terminology\\n\\n| **Term** | **Definition** |\\n| :---: | --- |\\n| Cloud Provider Interface (CPI) | The Brains of the Provider. This is what makes the connection between the Kubernetes Cluster and the Cloud Provider |\\n| Container Storage Interface (CSI) | Using the CPI, this allows Kubernetes Storage Classes the ability to communicate with and utilize cloud tools. For example, vSphere Datastore |\\n| In-Tree Providers | Providers natively shipped with Kubernetes |\\n| Out-of-Tree Providers | Providers that are installed into Kubernetes after deployment |\\n\\n## Summary\\nWhile this blog was filled with dry information, it definitely helps describe what the heck is going on. In the next part of this blog, I will demonstrate the deployment of a K3S cluster in a vSphere environment.\\n\\n## References\\n\\n* [Rancher Labs Longhorn](https://longhorn.io/)\\n* [vSphere Cloud Provider](https://cloud-provider-vsphere.sigs.k8s.io/)\\n* [Rancher Labs K3S](https://rancher.com/docs/k3s/latest/en/)\\n* [Kuberenetes Cloud Controllers](https://kubernetes.io/docs/concepts/architecture/cloud-controller/        )"},{"id":"2021-07-03/k3s-cluster-with-vsphere-storage-part-ii","metadata":{"permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-ii","source":"@site/blog/2021-07-30-k3s-vsphere-part2/index.md","title":"K3S Cluster with vSphere Storage (Part II)","description":"K3S Cluster with vSphere Storage Hero","date":"2021-07-30T00:00:00.000Z","formattedDate":"July 30, 2021","tags":[{"label":"Rancher","permalink":"/tags/rancher"},{"label":"Kubernetes","permalink":"/tags/kubernetes"},{"label":"Storage","permalink":"/tags/storage"},{"label":"K3S","permalink":"/tags/k-3-s"},{"label":"VMWare","permalink":"/tags/vm-ware"}],"readingTime":7.24,"hasTruncateMarker":true,"authors":[{"name":"Peter Keech","title":"DevOps Engineer","url":"https://github.com/pkeech","imageURL":"https://github.com/pkeech.png","key":"Peter"}],"frontMatter":{"slug":"2021-07-03/k3s-cluster-with-vsphere-storage-part-ii","title":"K3S Cluster with vSphere Storage (Part II)","authors":["Peter"],"tags":["Rancher","Kubernetes","Storage","K3S","VMWare"],"image":"k3s_vsphere_part2.png"},"prevItem":{"title":"K3S Cluster with vSphere Storage (Part I)","permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-i"},"nextItem":{"title":"K3S Cluster with vSphere Storage (Part III)","permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-iii"}},"content":"![ K3S Cluster with vSphere Storage Hero](k3s_vsphere_part2.png)\\n\\nContinuing on the discussion from [Part I](../2021-07-30-k3s-vsphere-part1/index.md), in this part we are going to deploy a K3S cluster on a vSphere environment.\\n\\n\x3c!--truncate--\x3e\\n\\n## Step 1: The Setup\\n\\n:::note\\nI\u2019m currently in the process of creating Terraform scripts to accomplish this deployment, but feel free to deploy you cluster per your favorite method. I will create a blog post describing my Terraform scripts and the headache that PhotonOS caused once I\u2019ve completed the Terraform Script.\\n:::\\n\\nThe first thing we need to do is setup some nodes to deploy our K3S cluster too. Because I like pain, I decided to do this with VMware\u2019s PhotonOS but you can use something much simpler like Ubuntu. \\n\\nFor this blog, I\u2019m deploying; 3x VMs to act as Server (Master) nodes, 3x VMs to act as Agents (Worker) nodes and 1x VM as a cluster load balancer. At a minimum, each Virtual Machine (VM) needs to have a static IP, be able to communicate to one another and be able to communicate to the vSphere server (IP or FQDN). For complete setup instructions, refer to Rancher\u2019s K3S documentation to find out what requirements your distribution of Linux needs to deploy K3S.\\n\\nThe hardware requirements for each node is going to be specific to what you wish to deploy within the cluster. Because workloads vary dramatically, it is very difficult to suggest hardware sizes. However, as a starting point, and assuming you have the hardware capacity, I typically start with 2 vCPUs, 4GB of RAM and 16GB HDD (Thin Provisioned) and grow from there.\\n\\n:::caution\\nIf you are using PhotonOS like I did, you will need to install AppArmor with the following command:\\n\\n`tdnf install -y apparmor-utils`\\n:::\\n\\nOnce you have deployed your Virtual Machines you should something similar to this. These IP addresses can be whatever suits your network as long as they can communicate to each other.\\n\\n| **Node Name** | **IP Address** | **Description**\\n| :---: | :---: | :---: |\\n| MASTER-001 | 10.0.15.201 | Server (Master) Node |\\n| MASTER-002 | 10.0.15.202 | Server (Master) Node |\\n| MASTER-003 | 10.0.15.203 | Server (Master) Node |\\n| WORKER-001 | 10.0.15.204 | Agent (Worker) Node |\\n| WORKER-002 | 10.0.15.205 | Agent (Worker) Node |\\n| WORKER-003 | 10.0.15.206 | Agent (Worker) Node |\\n| LB-001 | 10.0.15.200 | NGINX Load Balancer |\\n\\n## Step 2: Deploying the External Load Balancer\\nTo make life a whole lot easier, and to follow good practices, we are going to deploy an external load balancer for the cluster. In a production environment this can be a physical appliance such as an F5, but in this example we are going to deploy NGINX. This Load Balancer is going to be configured to direct all Kubernetes Management traffic to the Server (Master) Nodes, and all the HTTP/HTTPS traffic to the Agent (Worker) Nodes.\\n\\n:::tip\\nIn a Homelab, or resource limited, environment, instead of deploying an external loadbalancer, you can deploy [MetalLB](https://metallb.universe.tf) within you cluster.\\n:::\\n\\nIn the future, after the cluster and application(s) have been deployed, DNS records will be pointed to the IP address of this Load Balancer. That traffic will be router to the cluster where an Ingress Controller will route the traffic to the appropriate services and pods.\\n\\nDeploying the Load Balancer may vary depending upon your distribution of choice, but to keep this simple, all we are going to do is; install Docker on the node and deploy NGINX via Docker Compose. The following steps need to be performed on the Load Balancer node.\\n\\nFor PhotonOS, Docker is pre-installed. To utilize Docker on PhotonOS, you need to enable and start the Docker service.\\n\\n``` bash\\n## ENABLE DOCKER SERVICE\\nsystemctl enable docker.service\\n\\n## START DOCKER SERVICE\\nsystemctl start docker.service\\n```\\n\\n:::caution\\nAs of 30 July 2021, PhotonOS 4.0 (build 1526e30ba) ships with an older version of Docker (Docker version 19.03.15, build 99e3ed8). Because of this we will need to install Docker-Compose.\\n\\n``` bash\\n## INSTALL DOCKER-COMPOSE BINARY\\ncurl -L \\"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\\" -o /usr/local/bin/docker-compose\\n\\n## MAKE EXECUTABLE\\nchmod +x /usr/local/bin/docker-compose\\n```\\n:::\\n\\nNow that we have our dependencies taken care of, the last thing we need to do is create some configurations files. The first one is `docker-compose.yml` and it is simply deploying a NGINX image with the configuration file that we are about to write. Additionally, the configuration specifies that NGINX shall use the local configuration file (`nginx.conf`) and to expect traffic on ports; 80, 443, 6443.\\n\\n:::tip\\nDo NOT use \u201clatest\u201d tags in production. This is bad practice and can lead to unexpected outages and issues. By default, not specifying a tag, latest will be used. \\n:::\\n\\nGiven this is in a Homelab, we can ignore this best practice.\\n\\n``` yaml title=\\"docker-compose.yml\\"\\n## DEFINE VERSION\\nversion: \\"3.7\\"\\n\\n## DEFINE SERVICE\\nservices:\\n  nginx:\\n    image: nginx\\n    restart: always\\n    volumes:\\n      - ./nginx.conf:/etc/nginx/nginx.conf\\n    ports:\\n      - 80:80\\n      - 443:443\\n      - 6443:6443\\n```\\n\\nBefore we can start our NGINX image, we have to create the NGINX configuration. The file below is a simple configuration file that routes all HTTP/HTTPS traffic to the Agent (Worker) nodes and all Kubernetes Management traffic the Server (Master) nodes.\\n\\nUpdate the IP addresses to reflect the addresses you have specified in your environment.\\n\\n``` title=\\"nginx.conf\\"\\nworker_processes 4;\\nworker_rlimit_nofile 40000;\\n\\nevents {\\n    worker_connections 8192;\\n}\\n\\nstream {\\n    upstream servers_http {\\n        least_conn;\\n        server 10.0.15.204:80 max_fails=3 fail_timeout=5s;\\n        server 10.0.15.205:80 max_fails=3 fail_timeout=5s;\\n        server 10.0.15.206:80 max_fails=3 fail_timeout=5s;\\n    }\\n    server {\\n        listen 80;\\n        proxy_pass servers_http;\\n    }\\n\\n    upstream servers_https {\\n        least_conn;\\n        server 10.0.15.204:443 max_fails=3 fail_timeout=5s;\\n        server 10.0.15.205:443 max_fails=3 fail_timeout=5s;\\n        server 10.0.15.206:443 max_fails=3 fail_timeout=5s;\\n    }\\n    server {\\n        listen     443;\\n        proxy_pass servers_https;\\n    }\\n\\n    upstream api_server {\\n      server 10.0.15.201:6443;\\n      server 10.0.15.202:6443;\\n      server 10.0.15.203:6443;\\n    }\\n\\n    server {\\n      listen 6443;\\n      proxy_pass api_server;\\n      proxy_timeout 30;\\n      proxy_connect_timeout 2s;\\n    }\\n}\\n```\\n\\nNow we have both of our configuration files, the last thing to do is start the NGINX instance. To do this we run `docker-compose up -d`. This will start the NGINX instance in a detached state.\\n\\n## Step 3: Deploying the Cluster\\nNow that we have our external Load Balancer in place, we can turn our attentions to the cluster.\\n\\n### Configure Primary Server (Master) Node\\nThe first step in creating a cluster is deploying our primary Server (Master) node and initiating the Kubernetes cluster.\\n\\n1. SSH into the primary Server (Master) Node\\n2. Create a `k3s-config.yaml` file\\n\\n``` yaml title=\\"k3s-config.yaml\\"\\ncluster-init: true\\ndisable-cloud-controller: true\\nwrite-kubeconfig-mode: \\"0644\\"\\ntls-san:\\n  - \\"10.0.15.200\\"\\ndisable:\\n  - traefik\\nkubelet-arg:\\n  - \\"cloud-provider=external\\"\\n```\\n\\n3. Install the Primary Master Node\\n\\n``` bash\\ncurl -sfL https://get.k3s.io | sh -s - server --disable-cloud-controller --config /root/k3s-config.yaml\\n```\\n\\n4. Obtain the Cluster token for use in future steps\\n\\n``` bash\\ncat /var/lib/rancher/k3s/server/node-token\\n```\\n\\n### Configure additional Server (Master) Nodes\\nFor each of the remaining Server (Master) Nodes, follow the following steps;\\n\\n1. SSH into the Node\\n2. Save the Server Token as an environment variable.\\n``` bash\\nexport K3S_TOKEN=\'{{ CLUSTER_TOKEN }}\'\\n```\\n3. Install K3S as a Server\\n``` bash\\ncurl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\\"--no-deploy traefik\\" sh -s - server --disable-cloud-controller --server https://10.0.15.200:6443 --token $K3S_TOKEN\\n```\\n\\n### Configure Agents (Worker) Nodes\\nFinally, lets add the Agent (Worker) Nodes to the cluster.\\n\\n1. SSH into the Node\\n2. Save the Cluster Token as an environment variable\\n``` bash\\nexport K3S_TOKEN=\'<<CLUSTER_TOKEN>>\'\\n```\\n3. Install K3S as a Agent\\n``` bash\\ncurl -sfL https://get.k3s.io | sh -s - agent --server https://10.0.15.200:6443 --token $K3S_TOKEN\\n```\\n\\n## Step 4: Configure Local Workstation\\nTo make management easier, we can configure your local workstation to manage the cluster. This removes the need to SSH into different machines.\\n\\n### Prerequisites\\nTo enable your local workstation the ability to manage the cluster, please ensure the workstation has `kubectl` installed. On a Mac, you can use `brew install kubectl`. For other operating systems you can follow the Kubernetes documentation; For [Windows](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/), and for [Linux](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/).\\n\\n### Configuration Steps\\n\\n1. Copy kubeconfig file from Primary Server (Master) Node onto your local workstation (Replace the IP address with the IP of your primary Server (Master) Node.)\\n``` bash\\nscp root@10.0.15.201:/etc/rancher/k3s/k3s.yaml k3s.yaml\\n```\\n2. By default, the generated `KUBECONFIG` file (`k3s.yaml`) is populated with `127.0.0.1:6443` as the server address. You will need to update this file to reflect your external load balancer IP Address.\\n3. Finally, configure your Terminal to utilize the new kubeconfig. \\n``` bash\\nexport KUBECONFIG=~/k3s.yaml\\n```\\n\\n## Summary\\nHaving followed this blog post, you should now have the framework of the cluster up and running. Before we can start deploying workloads there are a few more steps that we need accomplish.\\n\\n## References\\n\\n* [Rancher Labs Longhorn](https://longhorn.io/)\\n* [vSphere Cloud Provider](https://cloud-provider-vsphere.sigs.k8s.io/)\\n* [Rancher Labs K3S](https://rancher.com/docs/k3s/latest/en/)\\n* [Kuberenetes Cloud Controllers](https://kubernetes.io/docs/concepts/architecture/cloud-controller/)\\n* [VMware Photon OS](https://vmware.github.io/photon/docs/)\\n* [NGINX Load Balancer](http://nginx.org/en/docs/http/load_balancing.html)"},{"id":"2021-07-03/k3s-cluster-with-vsphere-storage-part-iii","metadata":{"permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-iii","source":"@site/blog/2021-07-30-k3s-vsphere-part3/index.md","title":"K3S Cluster with vSphere Storage (Part III)","description":"K3S Cluster with vSphere Storage Hero","date":"2021-07-30T00:00:00.000Z","formattedDate":"July 30, 2021","tags":[{"label":"Rancher","permalink":"/tags/rancher"},{"label":"Kubernetes","permalink":"/tags/kubernetes"},{"label":"Storage","permalink":"/tags/storage"},{"label":"K3S","permalink":"/tags/k-3-s"},{"label":"VMWare","permalink":"/tags/vm-ware"}],"readingTime":8.105,"hasTruncateMarker":true,"authors":[{"name":"Peter Keech","title":"DevOps Engineer","url":"https://github.com/pkeech","imageURL":"https://github.com/pkeech.png","key":"Peter"}],"frontMatter":{"slug":"2021-07-03/k3s-cluster-with-vsphere-storage-part-iii","title":"K3S Cluster with vSphere Storage (Part III)","authors":["Peter"],"tags":["Rancher","Kubernetes","Storage","K3S","VMWare"],"image":"k3s_vsphere_part3.png"},"prevItem":{"title":"K3S Cluster with vSphere Storage (Part II)","permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-ii"},"nextItem":{"title":"K3S Cluster with vSphere Storage (Part IV)","permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-iv"}},"content":"![ K3S Cluster with vSphere Storage Hero](k3s_vsphere_part3.png)\\n\\nHaving followed the steps in Part II, you should have a K3S Cluster stood up. In this Part we are going to add the all important Cloud Provider.\\n\\n\x3c!--truncate--\x3e\\n\\n## Step 1: Prepare Nodes for vSphere Cloud Provider\\nBefore we can install the vSphere Cloud provider, there are couple configurations that need to be applied. You can read more about these requirements by heading over to the [VMWare documentation](https://cloud-provider-vsphere.sigs.k8s.io/).\\n\\n### Taint Nodes\\nThe Cloud Provider is going to be installed on the Server (Master) Nodes. To ensure this happens correctly, we need to ensure the nodes are tainted correctly.\\n\\n:::caution\\nNot sure if this is a K3S quirk or just my luck, but additionally I had to reapply the Master and Worker roles to the Nodes for the DaemonSet to recognize the Role.\\n:::\\n\\n``` bash\\n## TAINT SERVER (MASTER) NODES\\nkubectl taint nodes --selector=\'node-role.kubernetes.io/master\' node-role.kubernetes.io/master=:NoSchedule\\n\\n## ADD ROLES TO SERVER (MASTER) NODES\\nkubectl label nodes --selector=\'node-role.kubernetes.io/master\' node-role.kubernetes.io/master= --overwrite\\n\\n## TAINT AGENT (WORKER) NODES\\nkubectl taint nodes --selector=\'!node-role.kubernetes.io/master\' node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule\\n\\n## ADD ROLES TO AGENT (WORKER) NODES\\nkubectl label nodes --selector=\'node-role.kubernetes.io/worker\' node-role.kubernetes.io/worker= --overwrite\\n```\\n\\nOnce you have applied the Taints, you can verify they applied successfully by running this command;\\n\\n``` bash\\n## VALIDATE TAINTS\\nkubectl describe nodes | egrep \\"Taints:|Name:\\"\\n```\\n\\nIf the taints applied successfully you should see all the Server (Master) Nodes with the Taint `node-role.kubernetes.io/master:NoSchedule` and all Agent (Worker) Nodes have the Taint `node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule`\\n\\n### Configure Virtual Machine Settings\\nAnother configuration item that needs to occur, is ensuring that each virtual machine\u2019s hard disk (vmdk) is assigned a unique identifier (UUID). The easiest way to interact with the virtual machine settings is to use a command line utility called `govc`.\\n\\n> govc is vSphere CLI provided by VMware and is built on top of govmoi. The CLI is designed to be a user friendly CLI alternative to the GUI and well suited for automation tasks. It also acts as a test harness for the govmomi APIs and provides working examples of how to use the APIs.\\n> \\n> \u2013 [*Govc Github*](https://github.com/vmware/govmomi/tree/master/govc)\\n\\nTo install the CLI on Mac, you can use Homebrew. To install govc run `brew install govc`. For other operating systems, please refer to the [documentation](https://github.com/vmware/govmomi/tree/master/govc).\\n\\n### GOVC Setup\\nBefore we can use the utility we have to specify how `govc` connects to vSphere. In Mac and Linux, this is accomplished by setting environment variables.\\n\\n``` bash\\n## CONFIGURE GOVC\\nexport GOVC_URL=\'{{ URL-FOR-VCSA }}\'\\nexport GOVC_USERNAME=\'administrator@vsphere.local\'\\nexport GOVC_PASSWORD=\'{{ PASSWORD-FOR-ABOVE-ACCOUNT }}\'\\nexport GOVC_INSECURE=1\\n```\\n\\n| **Variable** | **Description** |\\n| --- | --- |\\n| GOVC_URL | the URL for your vSphere (VCSA) instance |\\n| GOVC_USERNAME | the login account for vSphere |\\n| GOVC_PASSWORD | the password for the account specified |\\n| GOVC_INSECURE | used when self-signed certificates are in use on VCSA, suppresses certificate warnings |\\n\\n### Enable DiskUUID\\nAt this point you should be able to utilize `govc` to query vSphere. The first command you can run is `govc ls`. This will list the any DataCenters you have configured and folders. You will need to enable DiskUUID on all the virtual machines in the cluster and to do this you will need to know the path to the virtual machine.\\n\\nTo find the path use the `govc ls` command followed by a path. Start with leaving it blank and keep drilling down until you find the paths to your virtual machines. For the example below, my final govc command looked like; `govc ls /Homelab/vm/Applications/Demo/`\\n\\nEnable DiskUUID on the virtual machines by running the following command. Outline below is how I applied it in my stack.\\n\\n``` bash\\n## ADD DISK UUID FLAG\\ngovc vm.change -e=\\"disk.enableUUID=1\\" -vm=\'{{ PATH-TO-VM }}\'\\n\\n## EXAMPLE -- ADD DISK UUID \\ngovc vm.change -e=\\"disk.enableUUID=1\\" -vm=\'/Homelab/vm/Applications/Demo/MASTER-001\'\\ngovc vm.change -e=\\"disk.enableUUID=1\\" -vm=\'/Homelab/vm/Applications/Demo/MASTER-002\'\\ngovc vm.change -e=\\"disk.enableUUID=1\\" -vm=\'/Homelab/vm/Applications/Demo/MASTER-003\'\\ngovc vm.change -e=\\"disk.enableUUID=1\\" -vm=\'/Homelab/vm/Applications/Demo/WORKER-001\'\\ngovc vm.change -e=\\"disk.enableUUID=1\\" -vm=\'/Homelab/vm/Applications/Demo/WORKER-002\'\\ngovc vm.change -e=\\"disk.enableUUID=1\\" -vm=\'/Homelab/vm/Applications/Demo/WORKER-003\'\\n```\\n\\n### Adding ProviderID\\n\\nThe final configuration change needed is to add a \u201cProviderID\u201d to each of the nodes. Reading though the documentation, this ID needs to be unique but can be set to anything (within reason). Borrowing from the documentation, the easiest thing to do is to assign the virtual machine UUID as the ProviderID as these are always unique.\\n\\n``` bash\\n## ADD PROVIDERID TO EACH NODE\\nfor vm in $(govc ls /Homelab/vm/Applications/Demo/); do\\nMACHINE_INFO=$(govc vm.info -json -dc=Homelab -vm.ipath=\\"$vm\\" -e=true)\\nVM_NAME=$(jq -r \' .VirtualMachines[] | .Name\' <<< $MACHINE_INFO | awk \'{print tolower($0)}\')\\nVM_UUID=$( jq -r \' .VirtualMachines[] | .Config.Uuid\' <<< $MACHINE_INFO | awk \'{print toupper($0)}\')\\nkubectl patch node $VM_NAME.dodbrit.lab -p \\"{\\\\\\"spec\\\\\\":{\\\\\\"providerID\\\\\\":\\\\\\"vsphere://$VM_UUID\\\\\\"}}\\";\\ndone\\n```\\n\\nFor good measure, you can validate that the ProviderIDs were added correctly by running `kubectl describe nodes | egrep \\"ProviderID:|Name:\\"`. The expected outcome should be all of the nodes listed, with a ProviderID that starts with \u201cvsphere\u201d. Additionally, the IDs should all be unique.\\n\\n## Step 2: Install vSphere CPI\\nNow that we have our cluster up and running, and the nodes configured with additional information, we can finally install the vSphere Cloud Provider.\\n\\nThe first step is generate the required configuration file (`vsphere.conf`) and credentials (`cpi-secret.yaml`).\\n\\n``` conf title=\\"vsphere.conf\\"\\n# vsphere.conf\\n\\n# Global properties in this section will be used for all specified vCenters unless overriden in VirtualCenter section.\\nglobal:\\n  # default https port\\n  port: 443\\n  # set insecureFlag to true if the vCenter uses a self-signed cert\\n  insecureFlag: true\\n  # settings for using k8s secret\\n  secretName: cpi-global-secret\\n  secretNamespace: kube-system\\n\\n# vcenter section\\nvcenter:\\n  # arbitrary name for cluster\\n  demo:\\n    # ip or fqdn of vcsa\\n    server: 10.0.15.5\\n    # vSphere Datacenter Name\\n    datacenters:\\n      - Homelab\\n    # secret with credentials\\n    secretName: cpi-secret\\n    secretNamespace: kube-system\\n```\\n\\n``` yaml title=\\"cpi-secret.yaml\\"\\n# cpi-secret.yaml\\n\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: cpi-secret\\n  namespace: kube-system\\nstringData:\\n  10.0.15.5.username: \\"administrator@vsphere.local\\"\\n  10.0.15.5.password: \\"{{ PASSWORD-TO-ACCOUNT }}\\"\\n```\\n\\nTo apply the vSphere configuration to the cluster, we will create a ConfigMap of the file. To do that you run the following command;\\n\\n``` bash\\nkubectl create configmap cloud-config --from-file=vsphere.conf --namespace=kube-system\\n```\\n\\nThis will create a ConfigMap in the `kube-system` namespace with all the items we defined. To add the login credentials, we just need to apply the Secret as that was defined as a Secret to begin with.\\n\\n``` bash\\nkubectl create -f cpi-secret.yaml\\n```\\n\\nAgain for peace of mind, we can validate these applied;\\n\\n``` bash\\n## ENSURE CONFIGMAP WAS CREATED\\nkubectl get configmap cloud-config --namespace=kube-system\\n\\n## ENSURE SECRET WAS CREATED\\nkubectl get secret cpi-secret --namespace=kube-system\\n```\\n\\nAfter you have verified that the ConfigMap and Secret was created, you can delete both of these files. These files have potentially sensitive information in them and its good practice to remove them.\\n\\nNow we can deploy all the components of the vSphere Cloud Provider. We are going to apply them directly from the vSphere GitHub repository.\\n\\n:::tip\\nIt is recommend that you review the files first and once you feel confident that the files are safe, you can then apply them\\n:::\\n\\n``` bash\\n## CREATE ROLES\\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/master/manifests/controller-manager/cloud-controller-manager-roles.yaml\\n\\n## CREATE ROLE BINDINGS\\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-vsphere/master/manifests/controller-manager/cloud-controller-manager-role-bindings.yaml\\n\\n## CREATE DAMEONSET\\nkubectl apply -f https://github.com/kubernetes/cloud-provider-vsphere/raw/master/manifests/controller-manager/vsphere-cloud-controller-manager-ds.yaml\\n```\\n\\nAnd we can check that the Cloud Provider deployed successfully. You should see a **vsphere-cloud-controller-manager** running on each of your master nodes.\\n\\n``` bash\\nkubectl get pods --all-namespaces\\n```\\n\\nCongratulations! We have just install the vSphere Cloud Provider within our cluster!\\n\\n## Step 3: Install vSphere CSI\\n\\nNow that the Cloud Provider has been installed, we can turn the attention to the Cloud Storage Interface (CSI). Just like the Cloud Provider, we need to create some configuration files. Modify the file outlined below and save it as `csi-vsphere.conf`.\\n\\n``` conf title=\\"csi-vsphere.conf\\"\\n[Global]\\ncluster-id = \\"k3s-cluster\\"\\nuser = \\"administrator@vsphere.local\\"\\npassword = \\"{{ PASSWORD-FOR-ACCOUNT }}\\"\\nport = \\"443\\"\\ninsecure-flag = \\"1\\"\\n\\n[VirtualCenter \\"{{ VCSA-IP-ADDRESS }}\\"]\\ndatacenters = \\"Homelab\\"\\n\\n[Workspace]\\nserver = \\"{{ VCSA-IP-ADDRESS }}\\"\\ndatacenter = \\"Homelab\\"\\ndefault-datastore = \\"{{ DEFAULT-VSPHERE-DATASTORE }}\\"\\nresourcepool-path = \\"{{ DATACENTER-NAME }}/Resources\\"\\nfolder = \\"kubernetes\\"\\n\\n[Disk]\\nscsicontrollertype = pvscsi\\n```\\n\\nThe default-datastore field in the configuration file is important enough that it is included in the file, but will not be used once we configure the Storage Class. The Storage Class grants us the ability to define where the persistent storage will be save. Additionally, multiple Storage Class\u2018s can be defined that will allow for more granular storage based upon the deployment.\\n\\nUnlike CPI, we are going to upload this configuration to Kubernetes via a Secret.\\n\\n``` bash\\nkubectl create secret generic vsphere-config-secret --from-file=csi-vsphere.conf --namespace=kube-system\\n```\\n\\nWhen then validate that the Secret created successfully \u2026\\n\\n``` bash\\nkubectl get secret vsphere-config-secret --namespace=kube-system\\n```\\n\\n:::tip\\nAt this point, you made delete csi-vsphere.conf as it contains sensitive information.\\n:::\\n\\nNow we can deploy all the components of the vSphere Storage Provider. We are going to apply them directly from the vSphere GitHub repository.\\n\\n``` bash\\n## DEFINE CLUSTER ROLES\\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/v2.2.0/manifests/v2.2.0/rbac/vsphere-csi-controller-rbac.yaml\\n\\n## DEFINE ROLE BINDINGS\\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/v2.2.0/manifests/v2.2.0/rbac/vsphere-csi-node-rbac.yaml\\n\\n## DEPLOY STORAGE DRIVERS (CONTROLLER)\\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/v2.2.0/manifests/v2.2.0/deploy/vsphere-csi-controller-deployment.yaml\\n\\n## DEPLOY STORAGE DRIVERS (DAMEONSET)\\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/v2.2.0/manifests/v2.2.0/deploy/vsphere-csi-node-ds.yaml\\n```\\n\\nWe can then validate the installation by checking the Pod deployment. It may take a few minutes for all of the pods to get created and start running.\\n\\n``` bash\\nkubectl get pods --namespace=kube-system\\n```\\n\\n## Summary\\nAt this point, and ensuring that everything went smoothly, you should have a Kubernetes cluster running with both vSphere CPI and CSI running and configured. In Part IV of this blog, we are going to define the storage class within the cluster and deploy a demo application.\\n\\n## References\\n\\n* [Rancher Labs Longhorn](https://longhorn.io/)\\n* [vSphere Cloud Provider](https://cloud-provider-vsphere.sigs.k8s.io/)\\n* [vSphere Storage Provider](https://vsphere-csi-driver.sigs.k8s.io/)\\n* [Rancher Labs K3S](https://rancher.com/docs/k3s/latest/en/)\\n* [Kuberenetes Cloud Controllers](https://kubernetes.io/docs/concepts/architecture/cloud-controller/)\\n* [VMware Photon OS](https://vmware.github.io/photon/docs/)\\n* [NGINX Load Balancer](http://nginx.org/en/docs/http/load_balancing.html)\\n* [Govc Documentation](https://github.com/vmware/govmomi/tree/master/govc)"},{"id":"2021-07-03/k3s-cluster-with-vsphere-storage-part-iv","metadata":{"permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-iv","source":"@site/blog/2021-07-30-k3s-vsphere-part4/index.md","title":"K3S Cluster with vSphere Storage (Part IV)","description":"K3S Cluster with vSphere Storage Hero","date":"2021-07-30T00:00:00.000Z","formattedDate":"July 30, 2021","tags":[{"label":"Rancher","permalink":"/tags/rancher"},{"label":"Kubernetes","permalink":"/tags/kubernetes"},{"label":"Storage","permalink":"/tags/storage"},{"label":"K3S","permalink":"/tags/k-3-s"},{"label":"VMWare","permalink":"/tags/vm-ware"}],"readingTime":5.835,"hasTruncateMarker":true,"authors":[{"name":"Peter Keech","title":"DevOps Engineer","url":"https://github.com/pkeech","imageURL":"https://github.com/pkeech.png","key":"Peter"}],"frontMatter":{"slug":"2021-07-03/k3s-cluster-with-vsphere-storage-part-iv","title":"K3S Cluster with vSphere Storage (Part IV)","authors":["Peter"],"tags":["Rancher","Kubernetes","Storage","K3S","VMWare"],"image":"k3s_vsphere_part4.png"},"prevItem":{"title":"K3S Cluster with vSphere Storage (Part III)","permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-iii"},"nextItem":{"title":"Rancher Academy","permalink":"/2021-04-16/rancher-academy"}},"content":"![ K3S Cluster with vSphere Storage Hero](k3s_vsphere_part4.png)\\n\\nHaving followed along with the previous 3 blog posts, you should have a cluster that is one small step away from demonstrating the storage capabilities. In this final part of the series, we will perform our final configurations and demonstrate the simplicity of our storage solution.\\n\\n\x3c!--truncate--\x3e\\n\\n:::note\\nAt the end of this post, your cluster isn\u2019t quite ready for a workload as it is still missing an Ingress Controller. While deploying an Ingress Controller is outside the scope of this post, I will briefly describe how to deploy NGINX as an IngressController at the end of this post to get you started. I plan on making a post about this later to get into more detail.\\n:::\\n\\n## Step 1: Define Storage Policy in vSphere\\nIn order for our StorageClass to know where to store our persistent data, we are going to create a simple Storage Policy in vSphere. For the purposes of this demo, we are going to create a simple **tag based** policy. Feel free to make additional tags if your situation requires it. The names can be changed to suit your needs, but remember what you called them as you will need them later.\\n\\n### vSphere Category\\n\\n1. Login to your vSphere instance.\\n2. Click the Menu navigation dropdown and select Tags & Custom Attributes.\\n3. Click Categories and Select New.\\n4. Give the Category the name Kubernetes Storage, select One Tag, and deselect all but Datastore.\\n5. Click Create.\\n\\n![Category Screenshot](category.png)\\n\\n### vSphere Tags\\n\\n1. Navigate back to Tags.\\n2. Click New and create a new tag with the Demo, using Kubernetes Storage as the Category.\\n3. Click Create.\\n\\n![Tags Screenshot](tags.png)\\n\\n### Tag vSphere Datastore\\nNow we need to apply our newly created tag to our Datastore. This task is as simple as navigating to the Datastore tab, clicking Summary section, and selecting Assign in the Tags card. Select the newly created Demo tag and click Assign on last time to close the dialog box.\\n\\n### vSphere Storage Policy\\nNow that we have created our tags, and assigned them to our Datastore, we can create our simple but effective Storage Policy. Feel free to customize this later to suit your needs.\\n\\n1. Click the Menu navigation dropdown and select Policy and Profiles.\\n2. On VM Storage Policies click Create to create a new Storage Policy.\\n3. Name the policy Demo and click Next (remember this name as it is needed in the StorageClass step).\\n4. For the Policy structure, select Enable tag based placement rules.\\n5. Click Next.\\n6. In this dialog, select the Category you created in the previous step and leave Usage Option with the defaulted Use storage tagged with.\\n7. Click Browse and select the appropriate Tag. Click Next.\\n8. Confirm that your tag rule worked by verifying the DataStore populated in this table. If they are correct click Next and then Finish.\\n\\n![Storage Screenshot](storage.png)\\n\\n## Step 2: Define Storage Class\\nThe StorageClass defines how Kubernetes handles requests for persistent storage. By defining the StorageClass it allows application deployed to the cluster the ability to request storage dynamically. While this may seem a little scary at first, it\u2019s not the wild west. Limits can still be placed on a per application or per user basis.\\n\\n``` yaml title=\\"vmware-sc.yml\\"\\n#vmware-sc.yml\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: vsphere\\n  annotations:\\n    #storageclass.kubernetes.io/is-default-class: \\"true\\"  # Optional\\nprovisioner: csi.vsphere.vmware.com\\n#allowVolumeExpansion: true  # Optional: only applicable to vSphere 7.0U1 and above\\nparameters:\\n  storagepolicyname: \\"Demo\\"  #Optional Parameter\\n```\\n\\nIn the above file;\\n\\n| **Attribute** | **Description** |\\n| --- | --- | \\n| metadata/name\\tDefines the name of the StorageClass. | This is the name used by your application deployments when requesting storage |\\n| metadata/annotations/storageclass.kubernetes.io/is-default-class | This is set to true if you wish this Storage Class to be set as the default for the cluster |\\n| provisioner | This tells Kubernetes to use the CSI driver we deployed in the previous blog |\\n| allowVolumeExpansion | This is in beta. For more information read the [documentation](https://vsphere-csi-driver.sigs.k8s.io/features/volume_expansion.html) |\\n| parameters/storagepolicyname | This is wait instructs the CSI to use the Storage Policy to find the correct Datastore |\\n\\nWe can now apply our StorageClass to the cluster and we can validated that it applied successfully;\\n\\n``` bash\\n## APPLY STORAGE CLASS\\nkubectl apply -f vmware-sc.yml\\n\\n## VALIDATE\\nkubectl get sc\\n```\\n\\nThis command should return two items; **local-path** and **vsphere**.\\n\\n### Demo: Usage of vSphere Storage\\nWithout going into a deep dive in the realm of storage, the different types, Block vs. File, I\u2019m going to demonstrate how to use Block Storage on vSphere. For the vast majority of Kubernetes workloads Block Storage will work great.\\n\\nIf you have a workload that requires File Storage, review the documentation here to view the additional items that are required in vSphere to make that work.\\n\\nAs previously mentioned, one of the benefits of using a StorageClass is the ability to dynamically provision persistent volumes.\\n\\n``` yaml title=\\"example-storage.yml\\"\\n## example-storage.yml\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: vsphere-demo-labels\\n  labels:\\n    app: mongoDB\\n    environment: test\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  resources:\\n    requests:\\n      storage: 5Gi\\n  storageClassName: vsphere\\n```  \\n\\nUsing the example-storage.yml file defined above, running `kubectl apply -f example-storage.yml` will result in a PersistentVolumeClaim being created and a matching VMDK within vSphere.\\n\\nTo view your Persistent Volume Claims, simply run `kubectl get pvc`. You may need to add `-n {{ NAMESPACE_NAME }}` if you are deploying your volume in a namespace other than default.\\n\\nAdditionally, you can view your Persistent storage from within vSphere. If you navigate to Storage -> Name of DataStore -> Monitor. You can then select Cloud Native Storage -> Container Volumes. This will list of all the volumes being used on that Datastore.\\n\\nYou can add, or remove, labels in the Metadata section of the example-storage.yml file. These tags will appear within the vSphere UI and allow for easier filtering and deciphering of what application is using what volume.\\n\\nThe final thing to mention is, while this only an example Volume Claim, when deploying a production application, the same principles apply. Your application should have a PersistentVolumeClaim , if not multiple, and the only updates you should have to do is add the StorageClassName and possibly tags (if they aren\u2019t already there).\\n\\n## Summary\\n\\nWhile there were quite a few steps to get your cluster configured for use with your vSphere Infrastructure, at the end of the day, the struggle should be worth it. By utilizing your vSphere infrastructure, you can leverage the same backup and restore procedures to backup your persistent data.\\n\\nThank you for taking the time to read my blog and bearing with me while I figure out this whole blogging thing. Please feel free to submit any comments below on ways to improve.\\n\\nCheers,\\n\\nPeter\\n\\n## References\\n\\n* [Rancher Labs Longhorn](https://longhorn.io/)\\n* [vSphere Cloud Provider](https://cloud-provider-vsphere.sigs.k8s.io/)\\n* [vSphere Storage Provider](https://vsphere-csi-driver.sigs.k8s.io/)\\n* [Rancher Labs K3S](https://rancher.com/docs/k3s/latest/en/)\\n* [Kuberenetes Cloud Controllers](https://kubernetes.io/docs/concepts/architecture/cloud-controller/)\\n* [VMware Photon OS](https://vmware.github.io/photon/docs/)\\n* [NGINX Load Balancer](http://nginx.org/en/docs/http/load_balancing.html)\\n* [Govc Documentation](https://github.com/vmware/govmomi/tree/master/govc)"},{"id":"2021-04-16/rancher-academy","metadata":{"permalink":"/2021-04-16/rancher-academy","source":"@site/blog/2021-04-16/index.md","title":"Rancher Academy","description":"Rancher Academy","date":"2021-04-16T00:00:00.000Z","formattedDate":"April 16, 2021","tags":[{"label":"Certifications","permalink":"/tags/certifications"},{"label":"Rancher","permalink":"/tags/rancher"},{"label":"Kubernetes","permalink":"/tags/kubernetes"}],"readingTime":0.955,"hasTruncateMarker":true,"authors":[{"name":"Peter Keech","title":"DevOps Engineer","url":"https://github.com/pkeech","imageURL":"https://github.com/pkeech.png","key":"Peter"}],"frontMatter":{"slug":"2021-04-16/rancher-academy","title":"Rancher Academy","authors":["Peter"],"tags":["Certifications","Rancher","Kubernetes"]},"prevItem":{"title":"K3S Cluster with vSphere Storage (Part IV)","permalink":"/2021-07-03/k3s-cluster-with-vsphere-storage-part-iv"},"nextItem":{"title":"Welcome","permalink":"/welcome"}},"content":"![Rancher Academy](RancherAcademy.png)\\n\\nGood afternoon and happy Friday! Thanks to the wonderful [Network Chuck](https://youtu.be/gwUz3E9AW0w), today is day one of this website. As a beginning post I thought I would start with a free DevOps certification.\\n\\n\x3c!--truncate--\x3e\\n\\n:::tip\\n\\nAs a quick update, I have successfully passed the Rancher Operator: Level One!\\n\\nAs I previously mentioned, despite the fact that this is a Rancher certificate that is obviously geared towards the Rancher product, I highly recommend this certification for anyone that is new to the *Kubernetes* world or needs a refresher for the matter.\\n\\nRancher Operator: Level One touches upon many of the core concepts of *Kubernetes* and is a great place to start.\\n\\n:::\\n\\nRancher is one of the leading companies when it comes to all things DevOps (they haven\u2019t paid me to say that). As a commitment to the community, late last year *Rancher* released a no cost certification for their Rancher platform. While the certification is about Rancher\u2019s cluster management tool, the certification is a great entry into the world of *Kubernetes*. \\n\\n[https://academy.rancher.com/](https://academy.rancher.com/)\\n\\nHopefully before my next post, I will be Rancher Operator certified! \\n\\nHave a great weekend everyone!\\n\\nCheers,\\n\\nPeter"},{"id":"welcome","metadata":{"permalink":"/welcome","source":"@site/blog/2021-04-15-Welcome.md","title":"Welcome","description":"Welcome!","date":"2021-04-15T00:00:00.000Z","formattedDate":"April 15, 2021","tags":[],"readingTime":0.645,"hasTruncateMarker":true,"authors":[{"name":"Peter Keech","title":"DevOps Engineer","url":"https://github.com/pkeech","imageURL":"https://github.com/pkeech.png","key":"Peter"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["Peter"]},"prevItem":{"title":"Rancher Academy","permalink":"/2021-04-16/rancher-academy"}},"content":"<div align=\\"center\\">\\n    <img src=\\"img/avatar.png\\" alt=\\"Avatar\\" style={{borderRadius: \'50%\' }}/>\\n</div>\\n\\n## Welcome!\\n\\nAs a quick introduction, my name is Peter Keech. I\u2019m a DevOps (DevSecOps) Engineer working in the government sector and a U.S. Air Force Veteran. I have a passion for all things DevOps and I am always learning new things about the technology.\\n\\n\x3c!--truncate--\x3e\\n\\nThe goal of this blog is to hopefully become a collection of lessons learnt throughout my journey in aiding the government in adopting DevOps policies and practices, as well as documenting my own growth along the way.\\n\\nHopefully, some of the lessons learnt here will help you in achieving the goals you have set out to complete.\\n\\nCheers for taking the time to visit my blog.\\n\\n<div align=\\"center\\">\\n    <img src=\\"img/dodbrit-logo.png\\" alt=\\"dodbrit-logo\\" width=\\"200px\\" height=\\"200px;\\"/>\\n</div>"}]}')}}]);